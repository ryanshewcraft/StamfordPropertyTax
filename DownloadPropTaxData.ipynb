{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas\n",
    "import requests\n",
    "from decimal import Decimal\n",
    "from re import sub\n",
    "from geopy.geocoders import Nominatim\n",
    "import string\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from one prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPropertyInfo(URL):\n",
    "    df = pd.DataFrame([])\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0'}\n",
    "    soup = BeautifulSoup(requests.get(URL,  headers=headers, verify=False).content, \"html.parser\")\n",
    "    \n",
    "    #2022 assement\n",
    "    try:\n",
    "        assess_2022 = soup.select_one(\"span[id=MainContent_lblGenAssessment]\").text\n",
    "        assess_2022 = Decimal(sub(r'[^\\d.]', '', assess_2022))\n",
    "        df = pd.concat([df, pd.DataFrame({'assess_2022':assess_2022}, index=[0])], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # 2021 \n",
    "    try:\n",
    "        table = soup.find('table', id='MainContent_grdHistoryValuesAsmt')\n",
    "        row = table.find_all('tr')[1]\n",
    "        columns = row.find_all('td')\n",
    "        assess_2021 = columns[3].text.strip()\n",
    "        assess_2021 = Decimal(sub(r'[^\\d.]', '', assess_2021))\n",
    "        df = pd.concat([df, pd.DataFrame({'assess_2021':assess_2021}, index=[0])], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # address\n",
    "    try:\n",
    "        addy = soup.select_one(\"span[id=MainContent_lblTab1Title]\").text + ', Stamford, CT'\n",
    "        df = pd.concat([df, pd.DataFrame({'address':addy}, index=[0])], axis=1)\n",
    "        geolocator = Nominatim(user_agent='stamford_prop_tax')\n",
    "        location = geolocator.geocode(addy)\n",
    "        df = pd.concat([df, pd.DataFrame({'longitude':location.longitude, 'latitude':location.latitude}, index=[0])], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #sale price and date\n",
    "    try:\n",
    "        sale_price = soup.select_one(\"span[id=MainContent_lblPrice]\").text\n",
    "        sale_price = float(Decimal(sub(r'[^\\d.]', '', sale_price)))\n",
    "        sale_date = soup.select_one(\"span[id=MainContent_lblSaleDate]\").text\n",
    "        df = pd.concat([df, pd.DataFrame({'sale_price':sale_price,'sale_date':sale_date}, index=[0])], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #year built and sq ft and acreage\n",
    "    try:\n",
    "        year_built = float(soup.select_one(\"span[id=MainContent_ctl01_lblYearBuilt]\").text)\n",
    "        sq_ft = float(soup.select_one(\"span[id=MainContent_ctl01_lblBldArea]\").text.replace(',',''))\n",
    "        table = soup.find('table', id='MainContent_tblLand')\n",
    "        row = table.find_all('tr')[0]\n",
    "        columns = row.find_all('td')\n",
    "        acres = float(columns[1].text.strip())\n",
    "        df = pd.concat([df, pd.DataFrame({'year_built':year_built, 'sq_ft':sq_ft, 'acreage':acres}, index=[0])], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # attributes\n",
    "    try:\n",
    "        table = soup.find('table', id='MainContent_ctl01_grdCns')\n",
    "        attributes = {}\n",
    "        for row in table.find_all('tr'):    \n",
    "            # Find all data for each column\n",
    "            columns = row.find_all('td')\n",
    "            if(columns != []):\n",
    "                col_name = columns[0].text.strip().replace(':','')\n",
    "                col_val = columns[1].text.strip().replace(':','')\n",
    "                attributes[col_name] = col_val\n",
    "        attributes = pd.DataFrame(data=attributes, index=[0]).replace('',np.nan)\n",
    "        df = pd.concat([df,attributes], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # building areas\n",
    "    try:\n",
    "        table = soup.find('table', id='MainContent_ctl01_grdSub')\n",
    "        building_areas = {}\n",
    "        for row in table.find_all('tr'):    \n",
    "            # Find all data for each column\n",
    "            columns = row.find_all('td')\n",
    "            if(columns != []):\n",
    "                col_name = columns[0].text.strip().replace(':','')\n",
    "                col_val = float(columns[2].text.strip().replace(',',''))\n",
    "                building_areas[col_name] = col_val\n",
    "        building_areas = pd.DataFrame(data=building_areas, index=[0]).replace('',np.nan)\n",
    "        building_areas = building_areas.iloc[:,:-1]\n",
    "        df = pd.concat([df,building_areas], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # extra \n",
    "    try:\n",
    "        table = soup.find('table', id='MainContent_grdXf')\n",
    "        extra_features = {}\n",
    "        for row in table.find_all('tr'):    \n",
    "            # Find all data for each column\n",
    "            columns = row.find_all('td')\n",
    "            if(columns != []):\n",
    "                col_name = columns[0].text.strip().replace(':','')\n",
    "                col_val = columns[2].text.strip().replace(' S.F','')\n",
    "                col_val = col_val.replace(' UNITS', '')\n",
    "                col_val = float(col_val)\n",
    "                if col_name not in extra_features.keys():\n",
    "                    extra_features[col_name] = col_val\n",
    "                else:\n",
    "                    extra_features[col_name] += col_val\n",
    "        extra_features = pd.DataFrame(data=extra_features, index=[0]).replace('',np.nan)\n",
    "        df = pd.concat([df,extra_features], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # outbuildings\n",
    "    try:\n",
    "        table = soup.find('table', id='MainContent_grdOb')\n",
    "        outbuildings = {}\n",
    "        for row in table.find_all('tr'):    \n",
    "            # Find all data for each column\n",
    "            columns = row.find_all('td')\n",
    "            if(columns != []):\n",
    "                col_name = columns[0].text.strip().replace(':','')\n",
    "                col_val = float(columns[4].text.strip().replace(' S.F.','').replace(' UNITS', ''))\n",
    "                if col_name not in outbuildings.keys():\n",
    "                    outbuildings[col_name] = col_val\n",
    "                else:\n",
    "                    outbuildings[col_name] += col_val\n",
    "        outbuildings = pd.DataFrame(data=outbuildings, index=[0]).replace('',np.nan)\n",
    "        df = pd.concat([df,outbuildings], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # property type\n",
    "    try:\n",
    "        table = soup.find('table', id='MainContent_tblLandUse')\n",
    "        row = table.find_all('tr')[1]\n",
    "        columns = row.find_all('td')\n",
    "        prop_type = columns[1].text.strip()\n",
    "        df = pd.concat([df, pd.DataFrame({'prop_type':prop_type}, index=[0])], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract props from street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractStreet(URL):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0'}\n",
    "    soup = BeautifulSoup(requests.get(URL,  headers=headers, verify=False).content, \"html.parser\")\n",
    "    \n",
    "    df_street = pd.DataFrame([])\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if 'Parcel' in a['href']:\n",
    "            parcel_url = 'https://gis.vgsi.com/stamfordct/'+a['href']\n",
    "            try:\n",
    "                df_parcel = extractPropertyInfo(parcel_url)\n",
    "                df_parcel = df_parcel.loc[:,~df_parcel.columns.duplicated()]\n",
    "                df_street = pd.concat([df_street, df_parcel], axis=0)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return df_street"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract street names from letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractLetter(letter):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (X11; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0'}\n",
    "    URL = 'https://gis.vgsi.com/stamfordct/Streets.aspx?Letter=%s' % letter\n",
    "    soup = BeautifulSoup(requests.get(URL,  headers=headers, verify=False).content, \"html.parser\")\n",
    "    \n",
    "    df_letter = pd.DataFrame([])\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if 'Streets.aspx?Name' in a['href']:\n",
    "            print('Street: %s' % a['href'].replace('Streets.aspx?Name=',''))\n",
    "            street_url = 'https://gis.vgsi.com/stamfordct/'+a['href']\n",
    "            df_street = extractStreet(street_url)\n",
    "            df_letter = pd.concat([df_letter, df_street], axis=0)\n",
    "\n",
    "    return df_letter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop over all letters an build output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Street: ABEL AVENUE\n",
      "Street: ABERDEEN STREET\n",
      "Street: ACOSTA STREET\n",
      "Street: ACRE VIEW DRIVE\n",
      "Street: ADAMS AVENUE\n",
      "Street: AKBAR ROAD\n",
      "Street: ALBERT PLACE\n",
      "Street: ALBIN ROAD\n",
      "Street: ALDEN STREET\n",
      "Street: ALEXANDRA DRIVE\n",
      "Street: ALFRED LANE\n",
      "Street: ALGONQUIN AVENUE\n",
      "Street: ALLISON ROAD\n",
      "Street: ALMA ROCK ROAD\n",
      "Street: ALPINE STREET\n",
      "Street: ALTON ROAD\n",
      "Street: ALVORD LANE\n",
      "Street: AMELIA PLACE\n",
      "Street: AMHERST COURT\n",
      "Street: AMHERST PLACE\n",
      "Street: ANDERSON STREET\n",
      "Street: ANDOVER ROAD\n",
      "Street: ANN STREET\n",
      "Street: ANNIE PLACE\n",
      "Street: ANTHONY STREET\n",
      "Street: APPLE TREE DRIVE\n",
      "Street: APPLE TREE LANE\n",
      "Street: APPLE VALLEY ROAD\n",
      "Street: APPLEBEE ROAD\n",
      "Street: AQUILA ROAD\n",
      "Street: ARBOR ROAD\n",
      "Street: ARCHER LANE\n",
      "Street: ARDEN LANE\n",
      "Street: ARDMORE ROAD\n",
      "Street: ARDSLEY ROAD\n",
      "Street: ARLINGTON ROAD\n",
      "Street: ARNOLD DRIVE\n",
      "Street: ARROW HEAD DRIVE\n",
      "Street: ARTHUR PLACE\n",
      "Street: ASHTON ROAD\n",
      "Street: ASPEN LANE\n",
      "Street: ATLANTIC STREET\n",
      "Street: AULDWOOD ROAD\n",
      "Street: AUSTIN AVENUE\n",
      "Street: AUTUMN LANE\n",
      "Street: AVERY STREET\n",
      "Street: AVON LANE\n",
      "Street: AYRES DRIVE\n",
      "Street: BAKER PLACE\n",
      "Street: BANGALL ROAD\n",
      "Street: BANK STREET\n",
      "Street: BARCLAY DRIVE\n",
      "Street: BARHOLM AVENUE\n",
      "Street: BARMORE DRIVE\n",
      "Street: BARMORE DRIVE EAST\n",
      "Street: BARMORE DRIVE WEST\n",
      "Street: BARN HILL ROAD\n",
      "Street: BARNCROFT ROAD\n",
      "Street: BARNES ROAD\n",
      "Street: BARNSTABLE LANE\n",
      "Street: BARRETT AVENUE\n",
      "Street: BARRY PLACE\n",
      "Street: BARTINA LANE\n",
      "Street: BARTLETT LANE\n",
      "Street: BATEMAN WAY\n",
      "Street: BAXTER AVENUE\n",
      "Street: BAYBERRIE DRIVE\n",
      "Street: BEACHVIEW DRIVE\n",
      "Street: BEAL STREET\n",
      "Street: BEDFORD STREET\n",
      "Street: BEECHWOOD ROAD\n",
      "Street: BEL AIRE DRIVE\n",
      "Street: BELDEN STREET\n",
      "Street: BELL STREET\n",
      "Street: BELLMERE AVENUE\n",
      "Street: BELLTOWN ROAD\n",
      "Street: BEND OF RIVER LANE\n",
      "Street: BENNETT STREET\n",
      "Street: BENNINGTON COURT\n",
      "Street: BENSTONE STREET\n",
      "Street: BENTWOOD DRIVE\n",
      "Street: BERGES AVENUE\n",
      "Street: BERKELEY STREET\n",
      "Street: BERRIAN ROAD\n",
      "Street: BERTMOR DRIVE\n",
      "Street: BETTS AVENUE\n",
      "Street: BIG OAK CIRCLE\n",
      "Street: BIG OAK LANE\n",
      "Street: BIG OAK ROAD\n",
      "Street: BIRCH STREET\n",
      "Street: BIRCHWOOD ROAD\n",
      "Street: BIRD SONG LANE\n",
      "Street: BITTERSWEET LANE\n",
      "Street: BLACHLEY ROAD\n",
      "Street: BLACK ROCK ROAD\n",
      "Street: BLACK TWIG PLACE\n",
      "Street: BLACKBERRY DRIVE\n",
      "Street: BLACKBERRY DRIVE EAST\n",
      "Street: BLACKWOOD LANE\n",
      "Street: BLUE RIDGE DRIVE\n",
      "Street: BLUE ROCK DRIVE\n",
      "Street: BLUE SPRUCE LANE\n",
      "Street: BLUEBERRY DRIVE\n",
      "Street: BON AIR AVENUE\n",
      "Street: BONNER STREET\n",
      "Street: BORGLUM STREET\n",
      "Street: BOULDER BROOK DRIVE\n",
      "Street: BOULDEROL ROAD\n",
      "Street: BOUTON CIRCLE\n",
      "Street: BOUTON STREET\n"
     ]
    }
   ],
   "source": [
    "# lazy mans way to get list of letters\n",
    "az_Upper = string.ascii_uppercase\n",
    "letters = []\n",
    "for i in az_Upper:\n",
    "    letters.append(i)\n",
    "    \n",
    "# run the loop\n",
    "t1 = time.time()\n",
    "df_property = pd.DataFrame([])\n",
    "for letter in letters:\n",
    "    df_letter = extractLetter(letter)\n",
    "    df_letter.to_csv('/Users/Ryan/Dropbox/Projects/StamfordPropTax_2022/ExtractedData/letter_%s.csv' % letter)\n",
    "    df_property = pd.concat([df_property, df_letter],axis=0)  \n",
    "t2 = time.time()\n",
    "\n",
    "df_letter.to_csv('/Users/Ryan/Dropbox/Projects/StamfordPropTax_2022/ExtractedData/allProperties.csv')\n",
    "print('Time: ' (t2-t1)/60/60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
